<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Valerio Gherardi" />


<title>Classical k-gram Language Models in R</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
margin-bottom: 0em;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Classical <span class="math inline">\(k\)</span>-gram Language Models in R</h1>
<h4 class="author">Valerio Gherardi</h4>



<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(kgrams)</span></code></pre></div>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p><code>kgrams</code> provides R users with a set of tools for
training, tuning and exploring <span class="math inline">\(k\)</span>-gram language models<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. It gives support for
a number of common Natural Language Processing (NLP) tasks: from the
basic ones, such as extracting (<em>tokenizing</em>) <span class="math inline">\(k\)</span>-grams from a text and predicting
sentence or continuation probabilities, to more advanced ones such as
computing language model perplexities <a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> and sampling sentences
according the language model’s probability distribution. Furthermore, it
supports many classical <span class="math inline">\(k\)</span>-gram
smoothing methods, including the well-known modified Kneser-Ney
algorithm, first described in <span class="citation">(Chen and Goodman
1999)</span>, and widely considered the best performing smoothing
technique for <span class="math inline">\(k\)</span>-gram models.</p>
<p><span class="math inline">\(k\)</span>-gram language models are
notoriously demanding from the space point of view, and many of the
toolkits available for <span class="math inline">\(k\)</span>-gram based
NLP employ various techniques and data structures to achieve the data
compression required by the large scales of industry (and, sometimes,
academic) applications (see <span class="citation">(Pibiri and Venturini
2019)</span> for a recent review). On the other hand, at such large
scales, <em>neural</em> language models are often the most economic and
best performing choice, and this is likely to become more and more so in
the future. In developing <code>kgrams</code>, I made no special attempt
at data compression, and <span class="math inline">\(k\)</span>-grams
and count estimates are stored in plain C++ STL hash-tables, which can
grow rapidly large as the size of corpora and dictionaries
increases.</p>
<p>On the other hand, most focus is put on providing a <em>fast</em>,
time efficient implementation, with intuitive interfaces for text
processing and for model evaluation, and a reasonably large choice of
pre-implemented smoothing algorithms, making <code>kgrams</code>
suitable for small- and medium-scale language model experiments, for
rapidly building baseline models, and for pedagogical purposes.</p>
<p>In my point of view, the interest in <span class="math inline">\(k\)</span>-gram language models is mainly
pedagogical, as they provide very simple algorithms (together with all
their limitations) for learning a natural language’s sentence
probability distribution. Nonetheless, and for the same reasons, <span class="math inline">\(k\)</span>-gram models can also provide a useful,
quick baseline for model building with more complex algorithms. An R
implementation of classical <span class="math inline">\(k\)</span>-gram
smoothing techniques is lacking at the time of writing, and the goal of
<code>kgrams</code> is to fill this gap.</p>
<p>In the following Sections, I illustrate the prototypical workflow for
<a href="#kgram_model">building a <span class="math inline">\(k\)</span>-gram language model</a> with
<code>kgrams</code>, show how to compute <a href="#probability">probabilities</a> and <a href="#perplexity">perplexities</a>, and (for the sake of fun!) <a href="#sampling">generate random text</a> at different temperatures.</p>
</div>
<div id="kgram_model" class="section level2">
<h2>Building a <span class="math inline">\(k\)</span>-gram language
model</h2>
<p>This section illustrates the typical workflow for building a <span class="math inline">\(k\)</span>-gram language model with
<code>kgrams</code>. In summary, this involves the following main
steps:</p>
<ol style="list-style-type: decimal">
<li>Load the training corpus, i.e. the text from which <span class="math inline">\(k\)</span>-gram frequencies are estimated.</li>
<li>Preprocess the corpus and tokenize sentences.</li>
<li>Store <span class="math inline">\(k\)</span>-gram frequency counts
from the preprocessed training corpus.</li>
<li>Build the final language model, by initializing its parameters and
computing auxiliary counts possibly required by the smoothing technique
employed.</li>
</ol>
<p>We illustrate all these steps in the following.</p>
<div id="step-1-loading-the-training-corpus" class="section level3">
<h3>Step 1: Loading the training corpus</h3>
<p><code>kgrams</code> offers two options for reading the text corpora
used in its computations, which are basically in-memory and
out-of-memory solutions:</p>
<ul>
<li><em>in-memory</em>. The corpus is simply loaded in the R session as
a <code>character</code> vector.</li>
<li><em>out-of-memory</em>. The text is read in batches of fixed size
from a <code>connection</code>. This solution includes, for instance,
reading text from a file, from an URL, or from the standard input.</li>
</ul>
<p>The out-of-memory solution can be useful for training over large
corpora without the need of storing the entire text into the RAM.</p>
<p>In this vignette, for illustration, we will use the example dataset
<code>kgrams::much_ado</code> (William Shakespeare’s “Much Ado About
Nothing”).</p>
</div>
<div id="step-2-preprocessing-and-tokenizing-sentences" class="section level3">
<h3>Step 2: preprocessing and tokenizing sentences</h3>
<p>One can (and usually should) apply some transformations to the raw
training corpus before feeding it as input to the <span class="math inline">\(k\)</span>-gram counting algorithm. In particular,
the algorithm considers as a <em>sentence</em> each entry of its
pre-processed input, and pads each sentence with Begin-Of-Sentence (BOS)
and End-Of-Sentence (EOS) tokens. It considers as a <em>word</em> any
substring of a sentence delimited by (one or more) space characters.</p>
<p>For the moment, we only need to define the functions used for
preprocessing and sentence tokenization. We will use the following
functions, which leverage on the basic utilities
<code>kgrams::preprocess()</code> and <code>kgrams::tknz_sent()</code>,
and perform some additional steps, since we will be reading raw HTML
lines from the URL connection created above.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>.preprocess <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>        <span class="co"># Remove speaker name and locations (boldfaced in original html)</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>        x <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">&quot;&lt;b&gt;[A-z]+&lt;/b&gt;&quot;</span>, <span class="st">&quot;&quot;</span>, x)</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>        <span class="co"># Remove other html tags</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>        x <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">&quot;&lt;[^&gt;]+&gt;||&lt;[^&gt;]+$||^[^&gt;]+&gt;$&quot;</span>, <span class="st">&quot;&quot;</span>, x)</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>        <span class="co"># Apply standard preprocessing including lower-case</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>        x <span class="ot">&lt;-</span> kgrams<span class="sc">::</span><span class="fu">preprocess</span>(x)</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>        <span class="co"># Collapse to a single string to avoid splitting into more sentences at the end of lines</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>        x <span class="ot">&lt;-</span> <span class="fu">paste</span>(x, <span class="at">collapse =</span> <span class="st">&quot; &quot;</span>)</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>        <span class="fu">return</span>(x)</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>}</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>.tknz_sent <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>        <span class="co"># Tokenize sentences</span></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>        x <span class="ot">&lt;-</span> kgrams<span class="sc">::</span><span class="fu">tknz_sent</span>(x, <span class="at">keep_first =</span> <span class="cn">TRUE</span>)</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>        <span class="co"># Remove empty sentences</span></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a>        x <span class="ot">&lt;-</span> x[x <span class="sc">!=</span> <span class="st">&quot;&quot;</span>]</span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a>        <span class="fu">return</span>(x)</span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="step-3-get-k-gram-frequency-counts" class="section level3">
<h3>Step 3: get <span class="math inline">\(k\)</span>-gram frequency
counts</h3>
<p>We can now obtain <span class="math inline">\(k\)</span>-gram
frequency counts from Shakespeare with a single command, using the
function <code>kgram_freqs()</code>. The following stores <span class="math inline">\(k\)</span>-gram counts for <span class="math inline">\(k\)</span>-grams of order less than or equal to
<span class="math inline">\(N = 5\)</span>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>freqs <span class="ot">&lt;-</span> <span class="fu">kgram_freqs</span>(much_ado, <span class="co"># Read Shakespeare&#39;s text from connection</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>                     <span class="at">N =</span> <span class="dv">5</span>, <span class="co"># Store k-gram counts for k &lt;= 5</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>                     <span class="at">.preprocess =</span> .preprocess,  <span class="co"># preprocess text</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>                     <span class="at">.tknz_sent =</span> .tknz_sent, <span class="co"># tokenize sentences</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>                     <span class="at">verbose =</span> <span class="cn">FALSE</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>                     )</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>freqs</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a><span class="co">#&gt; A k-gram frequency table.</span></span></code></pre></div>
<p>The object <code>freqs</code> is an object of class
<code>kgram_freqs</code>, i.e. a <span class="math inline">\(k\)</span>-gram frequency table. We can obtain a
first informative summary of what this object contains by calling
<code>summary()</code>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="fu">summary</span>(freqs)</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="co">#&gt; A k-gram frequency table.</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="co">#&gt; Parameters:</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a><span class="co">#&gt; * N: 5</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="co">#&gt; * V: 3046</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a><span class="co">#&gt; Number of words in training corpus:</span></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a><span class="co">#&gt; * W: 26123</span></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a><span class="co">#&gt; Number of distinct k-grams with positive counts:</span></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a><span class="co">#&gt; * 1-grams:3048</span></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a><span class="co">#&gt; * 2-grams:14373</span></span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a><span class="co">#&gt; * 3-grams:21201</span></span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a><span class="co">#&gt; * 4-grams:22705</span></span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a><span class="co">#&gt; * 5-grams:22974</span></span></code></pre></div>
<p>The parameter <code>V</code> is the size of the dictionary, which was
created behind the scenes by <code>kgram_freqs()</code>, using all words
encountered in the text. In alternative, we could have used a
pre-specified dictionary through the argument <code>dict</code>, and
specify whether new words (not present in the original dictionary)
should be added to it, or be replaced by an Unknown-Word (UNK) token, by
the argument <code>open_dict</code>; see <code>?kgram_freqs</code> for
further details. The number of distinct unigrams is greater than the
size of the dictionary, because the former also includes the special BOS
and EOS tokens.</p>
<p>Notice that the functions <code>.preprocess()</code> and
<code>.tknz_sent()</code> we defined above are passed as arguments of
<code>kgram_freqs()</code><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. These are also saved in the final
<code>kgram_freqs</code> object, and are by default applied also to
inputs at prediction time.</p>
<p>The following shows how to query <span class="math inline">\(k\)</span>-gram counts from the frequency table
created above <a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Query some simple unigrams and bigrams</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="fu">query</span>(freqs, <span class="fu">c</span>(<span class="st">&quot;leonato&quot;</span>, <span class="st">&quot;enter leonato&quot;</span>, <span class="st">&quot;thy&quot;</span>, <span class="st">&quot;smartphones&quot;</span>))</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="co">#&gt; [1] 38  6 52  0</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="co"># Query k-grams at the beginning or end of a sentence</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="fu">query</span>(freqs, <span class="fu">c</span>(<span class="fu">BOS</span>() <span class="sc">%+%</span> <span class="fu">BOS</span>() <span class="sc">%+%</span> <span class="st">&quot;i&quot;</span>, <span class="st">&quot;love&quot;</span> <span class="sc">%+%</span> <span class="fu">EOS</span>()))</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="co">#&gt; [1] 206   0</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a><span class="co"># Total number of words processed</span></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a><span class="fu">query</span>(freqs, <span class="st">&quot;&quot;</span>) </span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a><span class="co">#&gt; [1] 26123</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a><span class="co"># Total number of sentences processed</span></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a><span class="fu">query</span>(freqs, <span class="fu">EOS</span>()) </span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a><span class="co">#&gt; [1] 2219</span></span></code></pre></div>
<p>The most important use of <code>kgram_freqs</code> objects is to
create language models, as we illustrate in the next step.</p>
</div>
<div id="step-4.-build-the-final-language-model" class="section level3">
<h3>Step 4. Build the final language model</h3>
<p><code>kgrams</code> provides support for creating language models
using several classical smoothing techniques. The list of smoothers
currently supported by <code>kgrams</code> can be retrieved through:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="fu">smoothers</span>()</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="co">#&gt; [1] &quot;ml&quot;    &quot;add_k&quot; &quot;abs&quot;   &quot;kn&quot;    &quot;mkn&quot;   &quot;sbo&quot;   &quot;wb&quot;</span></span></code></pre></div>
<p>The documentation page <code>?smoothers</code> provides a list of
original references for the various smoothers. We will use <a href="https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing">Interpolated
Kneser-Ney</a> smoothing <span class="citation">(Kneser and Ney 1995;
see also Chen and Goodman 1999)</span>, which goes under the code
<code>&quot;kn&quot;</code>. We can get some usage help for this method through
the command:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="fu">info</span>(<span class="st">&quot;kn&quot;</span>)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="co">#&gt; Interpolated Kneser-Ney</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="co">#&gt;  * code: &#39;kn&#39;</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="co">#&gt;  * parameters: D</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="co">#&gt;  * constraints: 0 &lt;= D &lt;= 1</span></span></code></pre></div>
<p>As shown above, Kneser-Ney has one parameter <span class="math inline">\(D\)</span>, which is the discount applied to bare
<span class="math inline">\(k\)</span>-gram frequency counts or
continuation counts. We will initialize the model with <span class="math inline">\(D = 0.75\)</span>, and later tune this parameter
through a test corpus.</p>
<p>To train a language model with the <span class="math inline">\(k\)</span>-gram counts stored in
<code>freqs</code>, use:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>kn <span class="ot">&lt;-</span> <span class="fu">language_model</span>(freqs, <span class="st">&quot;kn&quot;</span>, <span class="at">D =</span> <span class="fl">0.75</span>)</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>kn</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="co">#&gt; A k-gram language model.</span></span></code></pre></div>
<p>This will create a <code>language_model</code> object, which can be
used to obtain word continuation and sentence probabilities. Let us
first get a summary of our final model:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="fu">summary</span>(kn)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="co">#&gt; A k-gram language model.</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a><span class="co">#&gt; Smoother:</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a><span class="co">#&gt; * &#39;kn&#39;.</span></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a><span class="co">#&gt; Parameters:</span></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a><span class="co">#&gt; * N: 5</span></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a><span class="co">#&gt; * V: 3046</span></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a><span class="co">#&gt; * D: 0.75</span></span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a><span class="co">#&gt; Number of words in training corpus:</span></span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a><span class="co">#&gt; * W: 26123</span></span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a><span class="co">#&gt; Number of distinct k-grams with positive counts:</span></span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a><span class="co">#&gt; * 1-grams:3048</span></span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a><span class="co">#&gt; * 2-grams:14373</span></span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a><span class="co">#&gt; * 3-grams:21201</span></span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a><span class="co">#&gt; * 4-grams:22705</span></span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a><span class="co">#&gt; * 5-grams:22974</span></span></code></pre></div>
<p>The parameter <code>D</code> can be accessed and modified through the
functions <code>parameters()</code> and <code>param()</code>, which have
a similar interface to the base R function <code>attributes()</code> and
<code>attr()</code>:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="fu">parameters</span>(kn)</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="co">#&gt; $N</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="co">#&gt; [1] 5</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a><span class="co">#&gt; $V</span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="co">#&gt; [1] 3046</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a><span class="co">#&gt; $D</span></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a><span class="co">#&gt; [1] 0.75</span></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a><span class="fu">param</span>(kn, <span class="st">&quot;D&quot;</span>)</span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a><span class="co">#&gt; [1] 0.75</span></span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a><span class="fu">param</span>(kn, <span class="st">&quot;D&quot;</span>) <span class="ot">&lt;-</span> <span class="fl">0.6</span></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a><span class="fu">param</span>(kn, <span class="st">&quot;D&quot;</span>)</span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a><span class="co">#&gt; [1] 0.6</span></span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a><span class="fu">param</span>(kn, <span class="st">&quot;D&quot;</span>) <span class="ot">&lt;-</span> <span class="fl">0.75</span></span></code></pre></div>
<p>We can also modify the order of the <span class="math inline">\(k\)</span>-gram model, by choosing any number less
than or equal to <span class="math inline">\(N = 5\)</span> (since we
stored up to <span class="math inline">\(5\)</span>-gram counts):</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="fu">param</span>(kn, <span class="st">&quot;N&quot;</span>) <span class="ot">&lt;-</span> <span class="dv">4</span> <span class="co"># &#39;kn&#39; uses only 1:4-grams</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="fu">param</span>(kn, <span class="st">&quot;N&quot;</span>)</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="co">#&gt; [1] 4</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="fu">param</span>(kn, <span class="st">&quot;N&quot;</span>) <span class="ot">&lt;-</span> <span class="dv">5</span> <span class="co"># &#39;kn&#39; uses also 5-grams</span></span></code></pre></div>
<p>In the next section we show how to use this language model for basic
tasks such as predicting word and sentence probabilities, and for more
complex tasks such as computing perplexities and generating random
text.</p>
</div>
</div>
<div id="using-language_model-objects" class="section level2">
<h2>Using <code>language_model</code> objects</h2>
<p>So far we have created a <code>language_model</code> object using
Interpolated Kneser-Ney as smoothing method. In this section we show how
to:</p>
<ul>
<li>Obtain word continuation and sentence probabilities.</li>
<li>Generate random text by sampling from the language model probability
distribution.</li>
<li>Compute the language model’s perplexity on a test corpus.</li>
</ul>
<div id="probability" class="section level3">
<h3>Word continuation and sentence probabilities</h3>
<p>We can obtain both sentence probabilities and word continuation
probabilities through the function <code>probability()</code>. This is
generic on the first argument, which can be a <code>character</code> for
sentence probabilities, or a <code>word_context</code> expression for
continuation probabilities.</p>
<p>Sentence probabilities can be obtained as follows (the first two are
sentences from the training corpus):</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="fu">probability</span>(<span class="fu">c</span>(<span class="st">&quot;Did he break out into tears?&quot;</span>,</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>              <span class="st">&quot;I see, lady, the gentleman is not in your books.&quot;</span>,</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>              <span class="st">&quot;We are predicting sentence probabilities.&quot;</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>              ),</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>            <span class="at">model =</span> kn</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>            )</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a><span class="co">#&gt; [1] 2.781389e-05 8.821088e-07 9.482178e-19</span></span></code></pre></div>
<p>Behind the scenes, the same <code>.preprocess()</code> and
<code>.tknz_sent()</code> functions used during training are being
applied to the input. We can turn off this behaviour by explicitly
specifying the <code>.preprocess</code> and <code>.tknz_sent</code>
arguments of <code>probability()</code>.</p>
<p>Word continuation probabilities are the conditional probabilities of
words following some given context. For instance, the probability that
the words <code>&quot;tears&quot;</code> or <code>&quot;pieces&quot;</code> will follow the
context <code>&quot;Did he break out into&quot;</code> are computed as
follows:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="fu">probability</span>(<span class="st">&quot;tears&quot;</span> <span class="sc">%|%</span> <span class="st">&quot;Did he break out into&quot;</span>, <span class="at">model =</span> kn)</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="co">#&gt; [1] 0.5813744</span></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="fu">probability</span>(<span class="st">&quot;pieces&quot;</span> <span class="sc">%|%</span> <span class="st">&quot;Did he break out into&quot;</span>, <span class="at">model =</span> kn)</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a><span class="co">#&gt; [1] 1.000375e-05</span></span></code></pre></div>
<p>The operator <code>%|%</code> takes as input a character vector on
its left-hand side, i.e. the list of candidate words, and a length one
character vector on its right-hand side, i.e. the given context. If the
context has more than <span class="math inline">\(N - 1\)</span> words
(where <span class="math inline">\(N\)</span> is the order of the
language model, five in our case), only the last <span class="math inline">\(N - 1\)</span> words are used for prediction.</p>
</div>
<div id="sampling" class="section level3">
<h3>Generating random text</h3>
<p>We can sample sentences from the probability distribution defined by
our language model using <code>sample_sentences()</code>. For
instance:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">840</span>)</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="fu">sample_sentences</span>(<span class="at">model =</span> kn, </span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>                 <span class="at">n =</span> <span class="dv">10</span>,</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>                 <span class="at">max_length =</span> <span class="dv">10</span></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>                 )</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a><span class="co">#&gt;  [1] &quot;i have studied officers ; &lt;EOS&gt;&quot;                                              </span></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a><span class="co">#&gt;  [2] &quot;truly by in your company thing that you ask for [...] (truncated output)&quot;     </span></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a><span class="co">#&gt;  [3] &quot;i protest i love the gentleman is wise ; &lt;EOS&gt;&quot;                               </span></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a><span class="co">#&gt;  [4] &quot;for it . &lt;EOS&gt;&quot;                                                               </span></span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a><span class="co">#&gt;  [5] &quot;the best befits can i for your own hobbyhorses hence [...] (truncated output)&quot;</span></span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a><span class="co">#&gt;  [6] &quot;but by this travail fit the length july cham&#39;s beard [...] (truncated output)&quot;</span></span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a><span class="co">#&gt;  [7] &quot;don pedro she doth well as being some attires and [...] (truncated output)&quot;   </span></span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a><span class="co">#&gt;  [8] &quot;exeunt all ladies only spots of grey all the wealth [...] (truncated output)&quot; </span></span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a><span class="co">#&gt;  [9] &quot;heighho ! &lt;EOS&gt;&quot;                                                              </span></span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a><span class="co">#&gt; [10] &quot;exit margaret ursula friar . &lt;EOS&gt;&quot;</span></span></code></pre></div>
<p>The sampling is performed word by word, and the output is truncated
if no <code>EOS</code> token is found after sampling
<code>max_length</code> words.</p>
<p>We can also sample with a temperature different from one. The
temperature transformation of a probability distribution <span class="math inline">\(p(i)\)</span> is defined by:</p>
<p><span class="math display">\[p_t(i) = \dfrac{\exp(\log{p(i)} / t)}
{Z(t)},\]</span> where <span class="math inline">\(Z(t)\)</span> is the
partition function, defined in such a way that <span class="math inline">\(\sum _i p_t(i) \equiv 1\)</span>. Intuitively,
higher and lower temperatures make the original probability distribution
smoother and rougher, respectively. By making a physical analogy, we can
think of less probable words as states with higher energies, and the
effect of higher (lower) temperatures is to make more (less) likely to
excite these high energy states.</p>
<p>We can test the effects of temperature on our Shakespeare-inspired
language model, by changing the parameter <code>t</code> of
<code>sample_sentences()</code> (notice that the default
<code>t = 1</code> corresponds to the original distribution):</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="fu">sample_sentences</span>(<span class="at">model =</span> kn, </span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>                 <span class="at">n =</span> <span class="dv">10</span>,</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>                 <span class="at">max_length =</span> <span class="dv">10</span>, </span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>                 <span class="at">t =</span> <span class="fl">0.1</span> <span class="co"># low temperature</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>                 )</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a><span class="co">#&gt;  [1] &quot;i will not have to do with you . &lt;EOS&gt;&quot;                                    </span></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a><span class="co">#&gt;  [2] &quot;i will go before and show him their examination . [...] (truncated output)&quot;</span></span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a><span class="co">#&gt;  [3] &quot;i will not be sworn but love may transform me [...] (truncated output)&quot;    </span></span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a><span class="co">#&gt;  [4] &quot;i will go get her picture . &lt;EOS&gt;&quot;                                         </span></span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a><span class="co">#&gt;  [5] &quot;i will go on the slightest errand now to the [...] (truncated output)&quot;     </span></span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a><span class="co">#&gt;  [6] &quot;i will not be sworn but love may transform me [...] (truncated output)&quot;    </span></span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a><span class="co">#&gt;  [7] &quot;i will not be sworn but love may transform me [...] (truncated output)&quot;    </span></span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a><span class="co">#&gt;  [8] &quot;and i will like a true drunkard utter all to [...] (truncated output)&quot;     </span></span>
<span id="cb15-14"><a href="#cb15-14" tabindex="-1"></a><span class="co">#&gt;  [9] &quot;i will not be sworn but love may transform me [...] (truncated output)&quot;    </span></span>
<span id="cb15-15"><a href="#cb15-15" tabindex="-1"></a><span class="co">#&gt; [10] &quot;i will not be sworn but love may transform me [...] (truncated output)&quot;</span></span>
<span id="cb15-16"><a href="#cb15-16" tabindex="-1"></a><span class="fu">sample_sentences</span>(<span class="at">model =</span> kn, </span>
<span id="cb15-17"><a href="#cb15-17" tabindex="-1"></a>                 <span class="at">n =</span> <span class="dv">10</span>,</span>
<span id="cb15-18"><a href="#cb15-18" tabindex="-1"></a>                 <span class="at">max_length =</span> <span class="dv">10</span>, </span>
<span id="cb15-19"><a href="#cb15-19" tabindex="-1"></a>                 <span class="at">t =</span> <span class="dv">10</span> <span class="co"># high temperature</span></span>
<span id="cb15-20"><a href="#cb15-20" tabindex="-1"></a>                 )</span>
<span id="cb15-21"><a href="#cb15-21" tabindex="-1"></a><span class="co">#&gt;  [1] &quot;minds wants valuing peace speech libertines after offered being braggarts [...] (truncated output)&quot;         </span></span>
<span id="cb15-22"><a href="#cb15-22" tabindex="-1"></a><span class="co">#&gt;  [2] &quot;smell fiveandthirty from possible knowest sickness tonight panders agony show&#39;d [...] (truncated output)&quot;   </span></span>
<span id="cb15-23"><a href="#cb15-23" tabindex="-1"></a><span class="co">#&gt;  [3] &quot;where&#39;s give intelligence princes finer tire scab brought rearward deserved [...] (truncated output)&quot;       </span></span>
<span id="cb15-24"><a href="#cb15-24" tabindex="-1"></a><span class="co">#&gt;  [4] &quot;heart&#39;s evening virtues holds c hadst persuasion can finer churchbench [...] (truncated output)&quot;            </span></span>
<span id="cb15-25"><a href="#cb15-25" tabindex="-1"></a><span class="co">#&gt;  [5] &quot;modesty thinks noncome remorse epitaphs consented mortifying whom hath expectation [...] (truncated output)&quot;</span></span>
<span id="cb15-26"><a href="#cb15-26" tabindex="-1"></a><span class="co">#&gt;  [6] &quot;impossible yielded deceive wedding mouth unclasp absentand qualify twelve giddily [...] (truncated output)&quot; </span></span>
<span id="cb15-27"><a href="#cb15-27" tabindex="-1"></a><span class="co">#&gt;  [7] &quot;certainly nightraven prized grief laugh claw invincible tyrant blessed run [...] (truncated output)&quot;        </span></span>
<span id="cb15-28"><a href="#cb15-28" tabindex="-1"></a><span class="co">#&gt;  [8] &quot;senseless beat time denies &#39;hundred ten forth hire&#39; reenter toothache [...] (truncated output)&quot;             </span></span>
<span id="cb15-29"><a href="#cb15-29" tabindex="-1"></a><span class="co">#&gt;  [9] &quot;laughed civil kill&#39;d mean hero&#39;s yea foundation deformed appetite hour [...] (truncated output)&quot;            </span></span>
<span id="cb15-30"><a href="#cb15-30" tabindex="-1"></a><span class="co">#&gt; [10] &quot;studied figure nine leonato enough ever herself confess authority sanctuary [...] (truncated output)&quot;</span></span></code></pre></div>
<p>As explained above, sampling with low temperature gives much more
weight to probable sentences, and indeed the output is very repetitive.
On the contrary, high temperature makes sentence probabilities more
uniform, and in fact our output above looks quite random.</p>
</div>
<div id="perplexity" class="section level3">
<h3>Compute language model’s perplexities</h3>
<p><a href="https://en.wikipedia.org/wiki/Perplexity">Perplexity</a> is
a standard evaluation metric for the overall performance of a language
model. It is given by <span class="math inline">\(P = e^H\)</span>,
where <span class="math inline">\(H\)</span> is the cross-entropy of the
language model sentence probability distribution against a test corpus
empirical distribution:</p>
<p><span class="math display">\[
H = - \dfrac{1}{W}\sum _s\ \ln (\text {Prob}(s))
\]</span> Here <span class="math inline">\(W\)</span> is total number of
words in the test corpus (following Ref. <span class="citation">(Chen
and Goodman 1999)</span>, we include counts of the EOS token, but not
the BOS token, in <span class="math inline">\(W\)</span>), and the sum
extends over all sentences in the test corpus. Perplexity does not give
direct information on the performance of a language model in a specific
end-to-end task, but is often found to correlate with the latter, which
provides a practical justification for the use of this metric. Notice
that better models are associated with lower perplexities, and that
<span class="math inline">\(H\)</span> is proportional to the negative
log-likelihood of the corpus under the language model assumption.</p>
<p>Perplexities can be computed in <code>kgrams</code> using the
function <code>perplexity()</code>, which can read text both from a
<code>character</code> vector and from a <code>connection</code>. We
will take our test corpus again from Shakespeare’s opus, specifically
the play “A Midsummer Night’s Dream”, which is example data from
<code>kgrams</code> namespace:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>midsummer[<span class="dv">840</span>]</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="co">#&gt; [1] &quot; when truth kills truth o devilishholy fray !&quot;</span></span></code></pre></div>
<p>We can compute the perplexity of our Kneser-Ney <span class="math inline">\(5\)</span>-gram model <code>kn</code> against this
corpus as follows:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="fu">perplexity</span>(midsummer, <span class="at">model =</span> kn)</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a><span class="co">#&gt; [1] 376.3205</span></span></code></pre></div>
<p>We can use perplexity to tune our model parameter <span class="math inline">\(D\)</span>. We compute perplexity over a grid of
values for <code>D</code> and plot the results. We do this for the <span class="math inline">\(k\)</span>-gram models with <span class="math inline">\(k \in \{2, 3, 4, 5\}\)</span>:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>D_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="fl">0.5</span>, <span class="at">to =</span> <span class="fl">0.99</span>, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>FUN <span class="ot">&lt;-</span> <span class="cf">function</span>(D, N) {</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>        <span class="fu">param</span>(kn, <span class="st">&quot;N&quot;</span>) <span class="ot">&lt;-</span> N</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>        <span class="fu">param</span>(kn, <span class="st">&quot;D&quot;</span>) <span class="ot">&lt;-</span> D</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>        <span class="fu">perplexity</span>(midsummer, <span class="at">model =</span> kn)</span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>}</span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>P_grid <span class="ot">&lt;-</span> <span class="fu">lapply</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>, <span class="cf">function</span>(N) <span class="fu">sapply</span>(D_grid, FUN, <span class="at">N =</span> N))</span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>oldpar <span class="ot">&lt;-</span> <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a><span class="fu">plot</span>(D_grid, P_grid[[<span class="dv">1</span>]], <span class="at">type =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;D&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Perplexity&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">300</span>, <span class="dv">500</span>))</span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a><span class="fu">lines</span>(D_grid, P_grid[[<span class="dv">1</span>]], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a><span class="fu">lines</span>(D_grid, P_grid[[<span class="dv">2</span>]], <span class="at">col =</span> <span class="st">&quot;chartreuse&quot;</span>)</span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a><span class="fu">lines</span>(D_grid, P_grid[[<span class="dv">3</span>]], <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a><span class="fu">lines</span>(D_grid, P_grid[[<span class="dv">4</span>]], <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<div class="figure">
<img role="img" aria-label="Perplexity as a function of the discount parameter of Interpolated Kneser-Ney 2-gram (red), 3-gram (green), 4-gram (blue) and 5-gram (black) models." src="data:image/png;base64,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" alt="Perplexity as a function of the discount parameter of Interpolated Kneser-Ney 2-gram (red), 3-gram (green), 4-gram (blue) and 5-gram (black) models." width="50%" />
<p class="caption">
Perplexity as a function of the discount parameter of Interpolated
Kneser-Ney 2-gram (red), 3-gram (green), 4-gram (blue) and 5-gram
(black) models.
</p>
</div>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="fu">par</span>(oldpar)</span></code></pre></div>
<p>We see that the optimal choices for <code>D</code> are close to its
maximum allowed value <code>D = 1</code>, for which the performance of
the 2-gram model is slightly worse than the higher order models, and
that the 5-gram model performs generally worse than the 3-gram and
4-gram models. Indeed, the optimized perplexities for the various <span class="math inline">\(k\)</span>-gram orders are given by:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="fu">sapply</span>(<span class="fu">c</span>(<span class="st">&quot;2-gram&quot;</span> <span class="ot">=</span> <span class="dv">1</span>, <span class="st">&quot;3-gram&quot;</span> <span class="ot">=</span> <span class="dv">2</span>, <span class="st">&quot;4-gram&quot;</span> <span class="ot">=</span> <span class="dv">3</span>, <span class="st">&quot;5-gram&quot;</span> <span class="ot">=</span> <span class="dv">4</span>),</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>       <span class="cf">function</span>(N) <span class="fu">min</span>(P_grid[[N]])</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>       )</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a><span class="co">#&gt;   2-gram   3-gram   4-gram   5-gram </span></span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a><span class="co">#&gt; 324.3317 320.0537 319.4006 319.4984</span></span></code></pre></div>
<p>which shows that the best performing model is the 4-gram one, while
it seems that the 5-gram model is starting to overfit (which is not very
surprising, given the ridiculously small size of our training
corpus!).</p>
</div>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>In this vignette I have shown how to implement and explore <span class="math inline">\(k\)</span>-gram language models in R using
<code>kgrams</code>. For further help, you can consult the reference
page of the <code>kgrams</code> <a href="https://vgherard.github.io/kgrams/">website</a>. Development of
<code>kgrams</code> takes place on its <a href="https://github.com/vgherard/kgrams/">GitHub repository</a>. If you
find a bug, please let me know by opening an issue on GitHub, and if you
have any ideas or proposals for improvement, please feel welcome to send
a pull request, or simply an e-mail at <a href="mailto:vgherard840@gmail.com" class="email">vgherard840@gmail.com</a>.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="unnumbered">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-chen1999empirical" class="csl-entry">
Chen, Stanley F, and Joshua Goodman. 1999. <span>“An Empirical Study of
Smoothing Techniques for Language Modeling.”</span> <em>Computer Speech
&amp; Language</em> 13 (4): 359–94.
</div>
<div id="ref-Kneser1995ImprovedBF" class="csl-entry">
Kneser, Reinhard, and H. Ney. 1995. <span>“Improved Backing-Off for
m-Gram Language Modeling.”</span> <em>1995 International Conference on
Acoustics, Speech, and Signal Processing</em> 1: 181–184 vol.1.
</div>
<div id="ref-Pibiri_2019" class="csl-entry">
Pibiri, Giulio Ermanno, and Rossano Venturini. 2019. <span>“Handling
Massive n -Gram Datasets Efficiently.”</span> <em>ACM Transactions on
Information Systems</em> 37 (2): 1–41. <a href="https://doi.org/10.1145/3302913">https://doi.org/10.1145/3302913</a>.
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>Here and below, when we talk about “language models”, we
always refer to <em>word-level</em> language models. In particular, a
<span class="math inline">\(k\)</span>-gram is a <span class="math inline">\(k\)</span>-tuple of words.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p><a href="https://en.wikipedia.org/wiki/Perplexity">Perplexity</a> is a
standard evaluation metric for language models, based on the model’s
sentence probability distribution cross-entropy with the empirical
distribution of a test corpus. It is described in some more detail in
this <a href="#perplexity">Subsection</a>.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Strictly speaking, a single argument
<code>.preprocess</code> would suffice, as the processed input is
(symbolically) <code>.tknz_sent(.preprocess(input))</code>. Having two
separate arguments for preprocessing and sentence tokenization has a
couple of advantages, as explained in <code>?kgram_freqs</code>.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>The string concatenation operator <code>%+%</code> is
equivalent to <code>paste(lhs, rhs)</code>. Also, the helpers
<code>BOS()</code>, <code>EOS()</code> and <code>UNK()</code> return the
BOS, EOS and UNK tokens, respectively.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
